{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Tool code and Experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Reads metadata and grading criteria from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def read_questions(file_path):\n",
    "    \"\"\"\n",
    "    Reads questions, correct answers, and max marks from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def read_student_answers(file_path):\n",
    "    \"\"\"\n",
    "    Reads all student data from processed file.\n",
    "    \"\"\"\n",
    "    return pd.read_excel(file_path).set_index(\"Student Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_content(metadata, rubric_level = \"full\"):\n",
    "    \"\"\"\n",
    "    Creates the system content based on metadata and grading rubric criteria (no, partial, full).\n",
    "    \"\"\"\n",
    "    if rubric_level == 'no':\n",
    "        prompt = \"\"\"\n",
    "        You are an AI assistant specialized in grading student exam answers.\n",
    "        Grade the student answers based on given correct answer and maximum marks.\n",
    "        \"\"\"\n",
    "    elif rubric_level == 'partial':\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant specialized in grading student exam answers based on specified criteria.\n",
    "        Evaluate answers provided by students based on the following exam instructions from the professor: \"{metadata['Grading remarks'][0]}\".\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant specialized in grading student exam answers based on specified criteria.\n",
    "        Evaluate answers provided by students based on provided correct answers and grading instructions.\n",
    "        You are grading for the subject: {metadata['Subject'][0]}, at the {metadata['Class Level'][0]} level \n",
    "        who are doing a major in {metadata['Major'][0]}, focusing on the following topics: {', '.join(metadata['Topics'])}.\n",
    "        Strongly consider the grading remarks from the professor for this particular exam: \"{metadata['Grading remarks'][0]}\"\n",
    "        \n",
    "        Additional Grading criteria to consider:\n",
    "        - Correctness: {metadata['Correctness'][0]}\n",
    "        - Argument Quality and Analysis: {metadata['Argument Quality and Analysis'][0]}\n",
    "        - Grammar: {metadata['Grammar'][0]}\n",
    "        - Clarity and Structure: {metadata['Clarity and Structure'][0]}\n",
    "\n",
    "        Based on the selected criterias marked 'Yes' and {metadata['Grading strictness'][0]} level of grading strictness, \n",
    "        create a detailed internal rubric for yourself to evaluate the student's response.\n",
    "        Consider it for grading and feedback of each student.\n",
    "        \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# - Grading strictness #-> very low, low, moderate, high, very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_question_prompt(question, correct_answer, student_answer, max_marks):\n",
    "    \"\"\"\n",
    "    Calls OpenAI API to grade a student's answer.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the passage text.\n",
    "    Question: \"{question}\"\n",
    "    \n",
    "    Correct Answer/guideline: \"{correct_answer}\"\n",
    "    \n",
    "    Student's Answer: \"{student_answer}\"\n",
    "    \n",
    "    Provide a grade out of {max_marks}, which can be fractional, and a very concise constructive feedback on the answer \n",
    "    if incorrect or only partially correct, explaining what was wrong. If right, just say \"Correct\" in the feedback.\n",
    "    Note - Donot reveal the exact grading criteria in the feedback.\n",
    "    \n",
    "    Follow response format:\n",
    "    Grade : \n",
    "    Feedback : \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_token_usage(total_tokens, file_path='token_usage.json'):\n",
    "    \"\"\"\n",
    "    Define the function to log token usage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize or update the JSON file\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        data = {\"total_tokens\": 0}\n",
    "\n",
    "    # Update the cumulative total\n",
    "    data[\"total_tokens\"] += total_tokens\n",
    "    \n",
    "    # Write back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(message_history, rubric_level, model_name, iter):\n",
    "    \"\"\"\n",
    "    Function to call OpenAI API and log token usage to a JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": message_history,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(api_endpoint, headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    print(response_data)\n",
    "    total_tokens = response_data['usage']['total_tokens']\n",
    "    \n",
    "    # Sanitize model name for file compatibility\n",
    "    safe_model_name = re.sub(r'[^A-Za-z0-9]', '_', model_name)\n",
    "    # Log the token usage\n",
    "    log_file = f\"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Output/token_usage_{safe_model_name}_{rubric_level}_{iter}.json\"\n",
    "    log_token_usage(total_tokens, log_file)\n",
    "\n",
    "    # Return the API response content\n",
    "    return response_data['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    \"\"\"\n",
    "    Parse the API response to extract grade and feedback\n",
    "    \"\"\"\n",
    "    grade = None\n",
    "    feedback = \"\"\n",
    "    for line in response.split('\\n'):\n",
    "        parts = [p.strip() for p in line.split(\":\", 1)]\n",
    "        if len(parts) == 2:\n",
    "            if parts[0].lower() == \"grade\":\n",
    "                # grade = parts[1]\n",
    "                try:\n",
    "                    grade = float(parts[1])\n",
    "                except ValueError:\n",
    "                    grade = None\n",
    "            elif parts[0].lower() == \"feedback\":\n",
    "                feedback = parts[1]\n",
    "    return {\"Grade\": grade, \"Feedback\": feedback}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_passage_questions(student_id, passage, questions_df, student_answers_df, system_context):\n",
    "def process_passage_questions(student_id, passage, questions_df, student_answers_df, system_context, rubric_level, model_name, iter):\n",
    "    \"\"\"\n",
    "    Process each passage with associated questions, updating message history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize message history with system context and passage text\n",
    "    message_history = [{\"role\": \"system\", \"content\": system_context}, \n",
    "                       {\"role\": \"user\", \"content\": f\"Following questions are based on the given passage text: {passage}\"},\n",
    "                       {\"role\": \"assistant\", \"content\": \"Understood\"}]\n",
    "    results = {}\n",
    "    \n",
    "    # Process each question under the same passage\n",
    "    for _, question_row in questions_df.iterrows():\n",
    "        question_id = question_row[\"Question ID\"]\n",
    "        student_answer = student_answers_df.loc[student_id, question_id]\n",
    "        \n",
    "        question_prompt = user_question_prompt(\n",
    "            question=question_row[\"Question\"],\n",
    "            correct_answer=question_row[\"Correct Answer\"],\n",
    "            student_answer=student_answer,\n",
    "            max_marks=question_row[\"Maximum Marks\"]\n",
    "        )\n",
    "\n",
    "        # Append question prompt to message history and call API\n",
    "        message_history.append({\"role\": \"user\", \"content\": question_prompt})\n",
    "        # response = call_openai_api(message_history)\n",
    "        response = call_openai_api(message_history, rubric_level, model_name, iter)\n",
    "        \n",
    "        # Parse and save response (grade and feedback)\n",
    "        parsed_response = parse_response(response)\n",
    "        results[question_id] = parsed_response\n",
    "        \n",
    "        # Append response to message history\n",
    "        message_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    return results, message_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_feedback(message_history, rubric_level, model_name, iter):\n",
    "    \"\"\"\n",
    "    Generate overall feedback for the student using full message history\n",
    "    \"\"\"\n",
    "\n",
    "    overall_feedback_prompt = \"\"\"\n",
    "    Considering all the data so far, generate an in-depth summary of the student’s performance, \n",
    "    highlighting key weaknesses and deeper knowledge gaps they can address to improve their subject understanding. \n",
    "    Limit feedback to 1-2 paragraphs and keep it very concise.\"\"\"\n",
    "    \n",
    "    message_history.append({\"role\": \"user\", \"content\": overall_feedback_prompt})\n",
    "    \n",
    "    # Save message history to a JSON file for analysis.\n",
    "    # file_path = \"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Output/message_history.json\"\n",
    "    # with open(file_path, \"w\") as file:\n",
    "    #     json.dump(message_history, file, indent=4)\n",
    "\n",
    "    return call_openai_api(message_history, rubric_level, model_name, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export results to Excel\n",
    "def export_results_to_excel(results, rubric_level, model_name, iter):\n",
    "    data = []\n",
    "    for student_id, student_data in results.items():\n",
    "        student_row = {\"Student ID\": student_id}\n",
    "        total_grade = 0\n",
    "\n",
    "        for passage_id, questions in student_data.items():\n",
    "            if passage_id != \"Overall Feedback\":\n",
    "                for question_id, result in questions.items():\n",
    "                    student_row[f\"{question_id}_Grade\"] = result[\"Grade\"]\n",
    "                    student_row[f\"{question_id}_Feedback\"] = result[\"Feedback\"]\n",
    "                    total_grade += result[\"Grade\"]\n",
    "            else:\n",
    "                student_row[\"Overall Feedback\"] = questions\n",
    "\n",
    "        student_row[\"Total_grade\"] = total_grade\n",
    "        data.append(student_row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Sanitize model name for file compatibility\n",
    "    safe_model_name = re.sub(r'[^A-Za-z0-9]', '_', model_name)\n",
    "    file_name = f\"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Output/student_grades_{safe_model_name}_{rubric_level}_{iter}.xlsx\"\n",
    "\n",
    "    df.to_excel(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_students(metadata_file, questions_file, student_answers_file, rubric_level = \"full\", model_name = \"gpt-4o-mini\", iter=1):\n",
    "    \"\"\"\n",
    "    Main function to execute grading for all students\n",
    "    \"\"\"\n",
    "\n",
    "    # Read data and setup\n",
    "    metadata = read_metadata(metadata_file)\n",
    "    questions_df = read_questions(questions_file)\n",
    "    student_answers_df = read_student_answers(student_answers_file)\n",
    "    system_context = create_system_content(metadata, rubric_level)\n",
    "    global_results = {}\n",
    "\n",
    "    # Process each student\n",
    "    for student_id in student_answers_df.index:\n",
    "        student_results = {}\n",
    "        complete_message_history = [{\"role\": \"system\", \"content\": system_context}]\n",
    "\n",
    "        # Process each passage\n",
    "        for passage_id, passage_questions in questions_df.groupby(\"Passage ID\"):\n",
    "            passage_text = passage_questions.iloc[0][\"Passage text\"] if pd.notnull(passage_id) else \"\"\n",
    "            # results, message_history = process_passage_questions(student_id, passage_text, passage_questions, student_answers_df, system_context)\n",
    "            results, message_history = process_passage_questions(student_id, passage_text, passage_questions, student_answers_df, system_context, rubric_level, model_name, iter)\n",
    "            student_results[passage_id] = results\n",
    "            \n",
    "            # Append passage's message history to the complete message history\n",
    "            complete_message_history.extend(message_history[1:])  # Skip the initial system context in each call\n",
    "\n",
    "        # Generate overall feedback using complete message history\n",
    "        overall_feedback = generate_overall_feedback(complete_message_history, rubric_level, model_name, iter)\n",
    "        student_results[\"Overall Feedback\"] = overall_feedback\n",
    "        global_results[student_id] = student_results\n",
    "    \n",
    "    # Convert global results to DataFrame and save to Excel\n",
    "    export_results_to_excel(global_results, rubric_level, model_name, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Files\n",
    "questions_file = \"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Input/questions.xlsx\"\n",
    "metadata_file = \"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Input/metadata.xlsx\"\n",
    "\n",
    "student_answers_file = \"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Input/compiled_student_answers.xlsx\"\n",
    "# student_answers_file = \"/Users/rrishabh/Documents/Thesis related docs/Thesis Data/Input/compiled_student_answers_test.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the experiment\n",
    "\n",
    "# rubric_levels = [\"full\", \"partial\", \"no\"]\n",
    "# models = [\"gpt-4o\", \"gpt-4o-mini\"]  # Add other relevant OpenAI models as needed\n",
    "iterations = 1 #4\n",
    "rubric_levels = [\"full\"]\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "# Loop through each combination of parameters\n",
    "for rubric_level in rubric_levels:\n",
    "    for model_name in models:\n",
    "        for iter_num in range(1, iterations + 1):\n",
    "            print(f\"Running experiment with rubric level: {rubric_level}, model: {model_name}, iteration: {iter_num}\")\n",
    "            grade_students(\n",
    "                metadata_file=metadata_file,\n",
    "                questions_file=questions_file,\n",
    "                student_answers_file=student_answers_file,\n",
    "                rubric_level=rubric_level,\n",
    "                model_name=model_name,\n",
    "                iter=iter_num\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------- Backup Cell --------------\n",
    "\n",
    "# def create_system_content(metadata, rubric_level = \"full\"):\n",
    "#     \"\"\"\n",
    "#     Creates the system content based on metadata and grading rubric criteria (no, partial, full).\n",
    "#     \"\"\"\n",
    "#     if rubric_level == 'no':\n",
    "\n",
    "#         prompt = \"\"\"\n",
    "#         You are an AI assistant specialized in grading student exam answers.\n",
    "#         Grade the student answers based on given correct answer and maximum marks.\n",
    "#         \"\"\"\n",
    "\n",
    "#     elif rubric_level == 'partial':\n",
    "       \n",
    "#         prompt = f\"\"\"\n",
    "#         You are an AI assistant specialized in grading student exam answers based on \n",
    "#         specified criteria.\n",
    "#         Evaluate answers provided by students based on the following exam instructions \n",
    "#         from the professor: \"{metadata['Grading remarks'][0]}\".\n",
    "#         \"\"\"\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         prompt = f\"\"\"\n",
    "#         You are an AI assistant specialized in grading student exam answers based on specified criteria.\n",
    "#         Evaluate answers provided by students based on provided correct answers.\n",
    "#         You are grading for the subject: {metadata['Subject'][0]}, at the {metadata['Class Level'][0]} level \n",
    "#         who are doing a major in {metadata['Major'][0]}, focusing on the following topics: {', '.join(metadata['Topics'])}.\n",
    "#         Strongly consider the grading remarks from the professor for this particular exam: \"{metadata['Grading remarks'][0]}\"\n",
    "        \n",
    "#         Additional Grading criteria to consider:\n",
    "#         - Completion: {metadata['Completion'][0]}\n",
    "#         - Correctness: {metadata['Correctness'][0]}\n",
    "#         - Argument Quality and Analysis: {metadata['Argument Quality and Analysis'][0]}\n",
    "#         - Originality and Creative: {metadata['Originality and Creative'][0]}\n",
    "#         - Grammar: {metadata['Grammar'][0]}\n",
    "#         - Clarity and Structure: {metadata['Clarity and Structure'][0]}\n",
    "#         - Length & Conciseness: {metadata['Length & Conciseness'][0]}\n",
    "#         - Evidence use: {metadata['Evidence use'][0]}\n",
    "#         - Grading strictness: {metadata['Grading strictness'][0]}\n",
    "\n",
    "#         Based on the selected criterias marked 'Yes' and given level of Grading strictness, \n",
    "#         create a detailed internal rubric for yourself to evaluate the student's response.\n",
    "#         Consider it for grading and feedback of each student.\n",
    "#         \"\"\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "# # - Grading strictness #-> very low, low, moderate, high, very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading instructions by professor.\n",
    "# For numerial questions, if the answer is within the range of +-1%, then give full designated marks, if its close, then partial marks, else 0\n",
    "#For theoretical questions, the sample answer consists of keywords/key arguments. Give full marks if 70% of the key arguments are discussed else give marks based on the proportional percentage of keywords/keyarguments present in the student answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
